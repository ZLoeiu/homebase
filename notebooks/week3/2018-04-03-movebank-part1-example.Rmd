---
title: "Summarizing Movebank data"
author: "Caryn Johansen"
date: "2018-04-05 00:00:00 +0300"
output: 
  html_document: 
    keep_md: yes
---

# Movebank

The next biological data source that we are going to investigate is [Movebank](https://www.movebank.org/). They say what they are best:

> We help animal tracking researchers to manage, share, protect, analyze, and archive their data. The animal tracking data in Movebank belongs to researchers all over the world who choose whether and how to share their data with the public.

Movebank is hosted by the [Max Planck Institude for Ornithology](http://www.orn.mpg.de/en), and, anecdotally, many of the data sets available are of birds species.
The general model of Movebank is that the researchers who do submit their data retain ownership and have a choice to make their data publicly available.
And, the owners of the data specify the terms of use.

This is a little different than some other databases I've seen, but also often the data involved can represent years of work in the field.
It also made accessing and interacting with the data on a bulk scale difficult and next to impossible, but we'll get to that.

The data in Movebank is **cool**. I was inspired by the book [Where Animals Go](http://wheretheanimalsgo.com/), which one my advisers in graduate school gave to me as a parting gift.
It's an incredible book and I was struck by how beautiful and informative the tracking data for animals was.
It's an intimate look at how animals behave, and presents some cool opportunities for data visualization, statistical analysis, and modeling.

The breakdown of accessing and analyzing Movebank data will take place in several parts.
Part I, this post, will discuss how to access and download data from Movebank, and show what type of data variables you can expect to find.
Later parts will discuss parsing date and time data, and increase the complexity of data analysis and modeling. 

## Become a Movebank user

In order to download data from Movebank directly, you must be a user. 
Become a user is free, and you can sign up [here](https://www.movebank.org/user/register).

To follow along with this series of posts, we will make the data available for the rest of the tutorial.

The data we're using falls under the general Movebank Terms of Use, which you can read [here](https://www.movebank.org/node/1934).
We are not using this data for any commercial use, and only intend it to show what Movebank has to offer, and to show some cool R libraries for data analysis and visualizations.

# Accessing Movebank Data

## Installing `move`

*Note*: If you just want to download the data and don't care about how we accessed Movebank, skip to the summarizing data section. We just use `move` to download data, and not for analysis.

To access data on movebank, you can interact with the [Movebank Tracking Data Map](https://www.movebank.org/panel_embedded_movebank_webapp), which is quite fun.


And there is also an R package that will interact with the website called, appropriately, [`move`](https://cran.r-project.org/web/packages/move/index.html). 

However, it has a dependency that was not straightforward for me to download. 
There is an R package called rgdal, which requires the software [GDAL](http://www.gdal.org/), Geospatial Data Abstraction Library.

To install this, I used [homebrew](https://brew.sh/), which is an incredible tool to install software if you are not familiar with it.

I used the brew recipe.  Type in your terminal (not in R):


```{}
brew install gdal
```

This worked well for me, and I was able to install `move` without a problem:

```{r eval=FALSE}
install.packages("rgdal")
install.packages("move")
```

Post any issues you have installing these libraries in the comments bellow and we'll try to help you out!

If everything has been installed correctly - load the move library:

```{r message=FALSE}
library(move)
```

# Getting to know `move`

As Ciera did with [neomata](link to post) and [taxize](link to post), I'm going to take a few moments to show what I learned while getting familiar with the library `move`.
This is a pretty big part of becoming good at certain functions and libraries - to read the manuals, do any provided or discovered tutorials, and to just *play* with the library.

`move` provides the ability to access Movebank, which is useful, and it also uses other libraries and the GDAL to analyze the GPS data, which is new to me.
Here's a [link to manual](https://cran.r-project.org/web/packages/move/vignettes/move.pdf). We'll be putting up a few posts about accessing, analyzing and visualizing the data here.

Create an object to store your Movebank login information. 

**we have to remember to X this out for the post! but you can feel free to use my login**

```{r}
loginStored <- movebankLogin(username = "caryn", password = "86sVZgNmyM")
```

The data in Movebank is organized by study, and researchers can upload their data from various different types of sensors.
We are going to be focusing on GPS sensors, but there are also accelerometers, radio transmitters, Argos Doppler, barometers, bind ring (I don't know what that is but it sounds cool), solar geolocator, and good ol' natural mark recording with human eyeballs.

It's useful to know the scope of the database you are searching. 
I initially thought I could get clever and get all the animals found in Movebank by getting all the study ids (with a combination of the functions `getMovebankStudies()` and `getMovebankID()`), but you cannot get around their data sharing policy that way.
Any study you may want to download, you must go online and agree to the terms for using that data before completing the download here in R.

But we can get basic information on all of the studies that allow some kind of access to their data using the `getMovebank()` function:

```{r}
all_studies <- getMovebank(entity_type = "study", login=loginStored)
dim(all_studies)
names(all_studies)
```

```{r}
head(all_studies$name)
```
Hmmm, looking at some of these study names, the authors of those studies don't seem to have practiced any type of repeating, standard naming procedure. That would make it difficult to parse out all animal names - and so could be a project for a later date.

Something we can do is map where these studies generally take place.

```{r fig.height=8, fig.width=12}
library(ggplot2)
world <- map_data("world")

ggplot() +
  geom_polygon(data = world,
               fill = "grey38",
               aes(x = long, y = lat, group = group)) +
  geom_point(data = all_studies, 
             aes(x = main_location_long, y = main_location_lat, color = as.factor(there_are_data_which_i_cannot_see))) +
  scale_color_discrete(guide = guide_legend(title = "Data Restrictions"))
```

The color of these points indicates if there are any restrictions on the availability of the data. Anything labeled false has all the data available.
There are `r sum(all_studies$there_are_data_which_i_cannot_see == "false")` studies that have all of their data available for download (after agreeing to the general Movebank privacy).

We can also examine the different types of sensors used in Movebank:

```{r}
getMovebank("tag_type", loginStored)
```

This returns a table with the different types of sensors, and a numeric tag assigned to each sensor. 
Unfortunately, at this time I don't see how to filter the studies by sensor type without first selecting a specific study. 
Which is a bummer, because I'm pretty curious about these bird-ring sensors, and would like to filter the studies based on that!

Besides filtering the downloaded database above, we can also use the `searchMovebankStudies()` function to search for terms in study names.

```{r}
searchMovebankStudies(x="coyote", login=loginStored)

head(searchMovebankStudies(x="oose", loginStored))

head(searchMovebankStudies(x="bird", login = loginStored))
```

So many studies to chose from!

Get the study ID to see how many animals are in the study.

```{r eval=FALSE, echo=FALSE}
ID <- getMovebankID(study = "Black-bellied Plover Alaska", login=loginStored)
head(getMovebankAnimals(study = ID, login = loginStored))
```

```{r}
ID <- getMovebankID(study = "Black-backed jackal, Etosha National Park, Namibia", login=loginStored)
head(getMovebankAnimals(study = ID, login = loginStored))
```

I like this study because it has many individuals, and their genders are identified. 
Many studies on Movebank are of a small number, one or two, and the information about the animals is not always available. 
Also, I've always kind of liked jackals, in a romantic, scrappy under-dog way. I really like coyotes, and they kind of seem like coyotes to me. Good enough! Let's download.

```{r eval=FALSE}
# be patient
jackals <- getMovebankData(study = "Black-backed jackal, Etosha National Park, Namibia", login = loginStored, removeDuplicatedTimestamps = TRUE)
```

```{r eval=TRUE, echo=FALSE}
jackals <- readRDS("../data/jackals_move_object.rds")
```

If you're following along, this was a pretty slow step for me.
Also, there is a warning about the option `removeDuplicatedTimestamps = TRUE`, however the data download fails if this is set to false.

This step returned a `move` object which seems to have a lot of excess information. Right now, all I want is information about the individuals they retrieved data from, and the location data over time, and the information to connect them. 


```{r}
idData <- jackals@idData
jackal_movement <- jackals@data
jackal_id_movement <- jackals@trackId
jackal_movement$local_identifier <- jackal_id_movement
str(jackal_movement)
```

This has created a data frame what has 130,686 rows, where each row contains the GPS location of an individual at a point in time.

Overall, I found interacting with the `move` library to be kind of frustrating. 
I really want to search through what a database has available and download a data frame without any bells and whistles. If you know what data set you want, going to the website and downloading the data through their data browsers gives you a flat comma separated file that is much easier to work with.

```{r eval=FALSE, echo=FALSE}
write.table(jackal_movement, file="../data/black-backed-jackal-Namibia.csv", quote = FALSE, row.names = FALSE, sep=",")
```


# Summarizing and cleaning data

```{r message=FALSE}
library(skimr)
```

Once you have acquired the data, the next step is to get a sense of what you can expect from the data, and if there are any missing values or weird-looking outlier data.

First, I usually like to get an idea of what types of values my data frame currently contains, and if I need to transform those values at all.

```{r}
str(jackal_movement)
```

My data frame is a mix of integers, numeric values, factors, and POSIXct values, which we will explain in more detail in a later post.

It's really important to check for missing values, and to get a sense of the range of values in your data set. As Ciera introduced in the previous posts, the `skimr` package is really wonderful for this.

```{r}
skim(jackal_movement)
```

`skim()` automatically splits up my data by variable type and summarizes each type, and packs a ton of information into the summary.
What I can quickly see is that I have no missing values for any of my variables - this is good! But you're not done quite yet.
Reading in data can make quite a few assumptions about what is and what is not labeled a missing value. 
Here, a missing value is an `NA`, but what if someone spelled out `not available` and put that into the data set? Or `missing` or just left it blank. We'll quickly check for these situations in a quick and dirty way.

```{r}
table(jackal_movement$local_identifier)
```

I use table to visually see what factors I have for a variable, and it's helpful to see the distribution of data for each factor. Here, we know that all of the local identifiers have some even and predictable naming scheme, that there are no totally mis-labeled factors, and that the data is not evenly distributed between the individual jackals. 
Some jackals had their GPS trackers on for much longer than others.

I'm not going to check the other factors with `table()`, because I can see that they only have one value and that it is complete.

I can also use the `skim()` results to to check on the integer and numeric values, and to get a sense of their distributions. 
Later, we're mainly going to be using local identifier, event id, and the latitude and longitude information.
Deployment id and tag id seem to be associated with a specific GPS device, and provides redundant information as the local identifier.
The sensor type is a single value, which probably means GPS in this situation. (Many data sets on Movebank have different types of sensors, so that is something to pay attention to if you are downloading other data sets.)

The event id is a sequential number, where each row of my data set has a unique event id. This will be incredibly useful later. 

I can use `range()` to verify what `skim()` appears to be showing for latitude and longitude - that there do not appear (at this level of analysis) to be weird ouliers.

```{r results="hold"}
range(jackal_movement$location_lat)
range(jackal_movement$location_long)
```

It should be said that Movebank likely has researchers clean their data before uploading it, which made this part pretty easy.
It's never safe to assume that your data is clean! Always check it out.

# Wrap Up

To wrap up this post, I'm going to do some pretty simple visualizations - because knowing you have a clean data set is pretty satisfying but not that visually compelling.

```{r message=FALSE}
library(tidyverse)
library(ggmap)
```

```{r results='hold'}
jackal_box <- make_bbox(lon = location_long, lat = location_lat, data=jackal_movement, f= 0.1)
jackal_map <- get_map(location = jackal_box, source = 'google', maptype = 'terrain')

ggmap(jackal_map) + geom_point(data=jackal_movement, aes(x=location_long, y=location_lat))
```

```{r}
ggmap(jackal_map) + geom_point(data=jackal_movement, aes(x=location_long, y=location_lat, color=local_identifier), alpha = 0.5)
```

In the next post, I will parse out the time stamp data and the fun will start with some analysis!

